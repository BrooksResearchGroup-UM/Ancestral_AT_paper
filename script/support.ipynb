{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the genome neighborhood analysis result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Read the test file into a dataframe\n",
    "df = pd.read_csv(\"../data/Other_Files/pfam_neighbors_PF02458.txt\", delimiter='\\t')\n",
    "df.to_csv(\"../output/pfam_neighbors_PF02458.csv\")\n",
    "\n",
    "# Save the query and neighbor IDs into two separate txt files for the SSN submission\n",
    "# Query\n",
    "Query_ID = []\n",
    "for id in df['Query ID'].to_list():\n",
    "    Query_ID.append(id)\n",
    "with open('../output/PF01494_with_PF02458.txt', 'w') as fp:\n",
    "    for id in Query_ID:\n",
    "        fp.write(\"%s\\n\" % id) # write each item on a new line\n",
    "\n",
    "# Neighbor\n",
    "Neighbor_ID = []\n",
    "for id in df['Neighbor ID'].to_list():\n",
    "    Neighbor_ID.append(id)\n",
    "with open('../output/PF02458_with_PF01494.txt', 'w') as fp:\n",
    "    for id in Neighbor_ID:\n",
    "        fp.write(\"%s\\n\" % id) # write each item on a new line\n",
    "\n",
    "# Create a \"PF01494 to PF02458\" dictionary and a \"PF02458 to PF01494\" dictionary and save them\n",
    "PF01494_to_PF02458_dict = {}\n",
    "PF02458_to_PF01494_dict = {}\n",
    "for i, j in zip(Query_ID, Neighbor_ID):\n",
    "    PF01494_to_PF02458_dict[i] = j\n",
    "    PF02458_to_PF01494_dict[j] = i\n",
    "\n",
    "with open(\"../output/PF01494_to_PF02458_dict.pkl\", 'wb') as file_handle:\n",
    "     pickle.dump(PF01494_to_PF02458_dict, file_handle)\n",
    "\n",
    "with open(\"../output/PF02458_to_PF01494_dict.pkl\", 'wb') as file_handle:\n",
    "     pickle.dump(PF02458_to_PF01494_dict, file_handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the clusterings of PF01494 and PF02458 by the AMI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs:  167\n",
      "AMI score:  0.8483212948915777\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "\n",
    "# Read the PF01494 clustering file\n",
    "df2 = pd.read_csv(\"../data/PF01494_SSN_Score150_cluster.txt\", delimiter='\\t')\n",
    "df2.to_csv(\"../output/PF01494_SSN_Score150_cluster.csv\")\n",
    "PF01494_cluster_id = df2['UniProt ID'].to_list()\n",
    "PF01494_cluster = df2['Cluster Number'].to_list()\n",
    "PF01494_id_to_cluster = {}\n",
    "for i, j in zip(PF01494_cluster_id, PF01494_cluster):\n",
    "    PF01494_id_to_cluster[i] = j\n",
    "    \n",
    "# Read the PF02458 clustering file\n",
    "df3 = pd.read_csv(\"../data/PF02458_SSN_Score150_cluster.txt\", delimiter='\\t')\n",
    "df3.to_csv(\"../output/PF02458_SSN_Score150_cluster.csv\")\n",
    "df3['UniProt ID'].to_list()\n",
    "PF02458_cluster_id = df3['UniProt ID'].to_list()\n",
    "PF02458_cluster = df3['Cluster Number'].to_list()\n",
    "PF02458_id_to_cluster = {}\n",
    "for n, k in zip(PF02458_cluster_id, PF02458_cluster):\n",
    "    PF02458_id_to_cluster[n] = k\n",
    "\n",
    "# Read the PF01494_and_PF02458 dictionary\n",
    "with open(\"../output/PF01494_to_PF02458_dict.pkl\", 'rb') as file_handle:\n",
    "    PF01494_to_PF02458_dict = pickle.load(file_handle)\n",
    "\n",
    "# Create a list for PF01494 to PF02458 pairs that are not singletons\n",
    "PF01494_and_PF02458 = []\n",
    "for id in PF01494_cluster_id:\n",
    "    if PF01494_to_PF02458_dict[id] in PF02458_cluster_id:\n",
    "        PF01494_and_PF02458.append(id)\n",
    "print(\"Total pairs: \", len(PF01494_and_PF02458))\n",
    "\n",
    "updated_PF01494_cluster = []\n",
    "updated_PF02458_cluster = []\n",
    "for PF01494_id in PF01494_and_PF02458:\n",
    "    updated_PF01494_cluster.append(PF01494_id_to_cluster[PF01494_id])\n",
    "    updated_PF02458_cluster.append(PF02458_id_to_cluster[PF01494_to_PF02458_dict[PF01494_id]])\n",
    "\n",
    "# Calculate the Adjusted Mutual Information (AMI) score\n",
    "AMI = adjusted_mutual_info_score(updated_PF01494_cluster, updated_PF02458_cluster)\n",
    "print(\"AMI score: \", AMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the active site residues between ancestral ATs and some extant ATs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chad/miniforge3/envs/ete3/lib/python3.10/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 7035\n",
      "  warnings.warn(\n",
      "/var/folders/jz/g6qwk6rs71nfx1y5xqrr_0j40000gn/T/ipykernel_44757/1579104604.py:204: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_cutomized = df_cutomized.append(pd.Series(sum_of_differences, index=df_cutomized.columns), ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Residue Number</th>\n",
       "      <th>Residue Distance</th>\n",
       "      <th>Anc451</th>\n",
       "      <th>Anc452</th>\n",
       "      <th>KAH6850668.1</th>\n",
       "      <th>KAH6641043.1</th>\n",
       "      <th>XP_001225293.1</th>\n",
       "      <th>Anc454</th>\n",
       "      <th>RYP17742.1</th>\n",
       "      <th>XP_046069174.1</th>\n",
       "      <th>KAF7554211.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>202.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>72.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>387.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>206.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Residue Number Residue Distance Anc451 Anc452 KAH6850668.1 KAH6641043.1  \\\n",
       "0            166.0              3.1      H      H            H            H   \n",
       "1            167.0              3.7      T      T            A            T   \n",
       "2             38.0              5.2      T      T            T            T   \n",
       "3            169.0              5.4      M      M            M            M   \n",
       "4            170.0              5.5      D      D            D            D   \n",
       "..             ...              ...    ...    ...          ...          ...   \n",
       "92           202.0             16.9      R      R            R            R   \n",
       "93            72.0             16.9      W      W            W            W   \n",
       "94           387.0             17.8      T      T            T            M   \n",
       "95           206.0             18.0      I      I            I            I   \n",
       "96             NaN              NaN    NaN    8.0         18.0         17.0   \n",
       "\n",
       "   XP_001225293.1 Anc454 RYP17742.1 XP_046069174.1 KAF7554211.1  \n",
       "0               H      H          H              H            H  \n",
       "1               T      T          T              T            T  \n",
       "2               T      T          T              T            T  \n",
       "3               M      M          M              M            M  \n",
       "4               D      D          D              D            D  \n",
       "..            ...    ...        ...            ...          ...  \n",
       "92              R      R          R              R            R  \n",
       "93              W      W          W              W            W  \n",
       "94              M      T          T              T            T  \n",
       "95              I      I          I              I            I  \n",
       "96           13.0    7.0        8.0           13.0         13.0  \n",
       "\n",
       "[97 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser, NeighborSearch\n",
    "\n",
    "AA_RESIDUES = [\n",
    "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',\n",
    "    'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP',\n",
    "    'TYR', 'VAL'\n",
    "]\n",
    "\n",
    "aa_dict = {\n",
    "    'ALA': 'A',  # Alanine\n",
    "    'ARG': 'R',  # Arginine\n",
    "    'ASN': 'N',  # Asparagine\n",
    "    'ASP': 'D',  # Aspartic acid\n",
    "    'CYS': 'C',  # Cysteine\n",
    "    'GLN': 'Q',  # Glutamine\n",
    "    'GLU': 'E',  # Glutamic acid\n",
    "    'GLY': 'G',  # Glycine\n",
    "    'HIS': 'H',  # Histidine\n",
    "    'ILE': 'I',  # Isoleucine\n",
    "    'LEU': 'L',  # Leucine\n",
    "    'LYS': 'K',  # Lysine\n",
    "    'MET': 'M',  # Methionine\n",
    "    'PHE': 'F',  # Phenylalanine\n",
    "    'PRO': 'P',  # Proline\n",
    "    'SER': 'S',  # Serine\n",
    "    'THR': 'T',  # Threonine\n",
    "    'TRP': 'W',  # Tryptophan\n",
    "    'TYR': 'Y',  # Tyrosine\n",
    "    'VAL': 'V'   # Valine\n",
    "}\n",
    "\n",
    "def is_amino_acid(residue):\n",
    "    return residue.get_resname() in AA_RESIDUES\n",
    "\n",
    "def get_centroid(residues):\n",
    "    coords = [atom.get_coord() for residue in residues for atom in residue]\n",
    "    active_site_center = np.mean(coords, axis=0)\n",
    "    return active_site_center\n",
    "\n",
    "def get_ligand_centroid(pdb_filename, ligand_resname):\n",
    "    pattern = r'(?<=\\s)-?\\d{1,3}\\.\\d{3}(?=\\s|$)'\n",
    "    with open(pdb_filename, \"r\") as file:\n",
    "        ligand_lines = [line.strip() for line in file if line.startswith(\"HETATM\") and ligand_resname in line]\n",
    "        coordinates = np.empty((0, 3))\n",
    "        for line in ligand_lines:\n",
    "            matched_strings = re.findall(pattern, line)\n",
    "            x_y_z = [float(x) for x in matched_strings[:3]]\n",
    "            coordinates = np.vstack((coordinates, x_y_z))\n",
    "    active_site_center = np.mean(coordinates, axis=0)\n",
    "    return active_site_center\n",
    "\n",
    "def find_residues_by_distance(atoms, center, distance):\n",
    "    neighbor_search = NeighborSearch(atoms)\n",
    "    nearby_atoms = neighbor_search.search(center, distance)\n",
    "    residues = set(atom.get_parent() for atom in nearby_atoms if is_amino_acid(atom.get_parent()))\n",
    "    return list(residues)\n",
    "\n",
    "def fasta_to_dict(fasta_file):\n",
    "    '''Reads a FASTA file and returns a dictionary with sequence IDs as keys and sequences as values'''\n",
    "    sequences = {}\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for record in SeqIO.parse(f, 'fasta'):\n",
    "            sequences[record.id] = str(record.seq)\n",
    "    return sequences\n",
    "\n",
    "def build_mapping(query_sequence):\n",
    "    '''Builds a dictionary mapping query positions to alignment positions'''\n",
    "    mapping = {}\n",
    "    count = 0\n",
    "    for i, res in enumerate(query_sequence):\n",
    "        if res != \"-\":\n",
    "            count += 1\n",
    "            mapping[count] = i\n",
    "    return mapping\n",
    "\n",
    "def get_corresponding_residues(sequences, mapping, query_positions):\n",
    "    '''Given a list of query positions, retrieves the residues in other sequences'''\n",
    "    residues = {}\n",
    "    for seq_id, seq in sequences.items():\n",
    "        residues[seq_id] = [seq[mapping[pos]] for pos in query_positions]\n",
    "    return residues\n",
    "\n",
    "# Alignment file:\n",
    "fasta_filename = '../data/CazE_anc451_clade_seqs_aligned.fasta' #@param {type:\"string\"}\n",
    "\n",
    "# Query seq:\n",
    "query_id = 'Anc451' #@param {type:\"string\"}\n",
    "\n",
    "# Structure file:\n",
    "pdb_filename = '../data/Anc451.pdb'\n",
    "\n",
    "# Select how to determine the center:\n",
    "center = 'key_residues' # \"ligand_substrate\" or \"key_residues\"\n",
    "\n",
    "# Select the chain of interest:\n",
    "chain_of_interest = 'A'\n",
    "\n",
    "# Ligand/substrate/cofactor name in the pdb file if \"ligand_substrate\" is selected:\n",
    "ligand_resname = '' \n",
    "\n",
    "# The chain where the ligand/substrate/cofactor is located at (A, B... or blank if unspecified):\n",
    "chain_of_ligand_substrate = 'A'\n",
    "\n",
    "# Select a targeted number of residues:\n",
    "num_residues = 96\n",
    "\n",
    "# Set the max distance to be a big number if you do not want to restrict the search by the max distance.\n",
    "max_distance = 30\n",
    "\n",
    "# Parse the PDB file\n",
    "parser = PDBParser()\n",
    "structure = parser.get_structure(\"protein\", pdb_filename)\n",
    "model = structure[0]\n",
    "\n",
    "if center == 'ligand_substrate':\n",
    "\n",
    "    if chain_of_ligand_substrate == '':\n",
    "        active_site_center = get_ligand_centroid(pdb_filename, ligand_resname)\n",
    "\n",
    "    else:\n",
    "        ligand = next((res for res in model[chain_of_ligand_substrate].get_residues() if res.get_resname() == ligand_resname), None)\n",
    "        active_site_center = get_centroid([ligand]) if ligand else sys.exit(\"Error: Ligand not found.\")\n",
    "\n",
    "else:\n",
    "    residue_numbers = input(\"Enter active site residue numbers (comma-separated): \").split(', ')\n",
    "    residues = [model[chain_of_interest][int(num.strip())] for num in residue_numbers]\n",
    "    active_site_center = get_centroid(residues)\n",
    "\n",
    "atoms = list(model[chain_of_interest].get_atoms())\n",
    "\n",
    "distance_step = 0.5\n",
    "current_distance = 5.0\n",
    "nearby_residues = find_residues_by_distance(atoms, active_site_center, current_distance)\n",
    "\n",
    "while len(nearby_residues) < num_residues and current_distance < max_distance:\n",
    "    current_distance += distance_step\n",
    "    nearby_residues = find_residues_by_distance(atoms, active_site_center, current_distance)\n",
    "\n",
    "if len(nearby_residues) >= num_residues:\n",
    "    residue_distances = [(res, np.linalg.norm(list(res.get_atoms())[0].get_coord() - active_site_center)) for res in nearby_residues]\n",
    "\n",
    "    # Sort the residue_distances based on the distance\n",
    "    sorted_residue_distances = sorted(residue_distances, key=lambda x: x[1])\n",
    "\n",
    "    # Extract the distances from the sorted_residue_distances\n",
    "    sorted_distances = [dist for _, dist in sorted_residue_distances]\n",
    "\n",
    "    # Get the required number of residues\n",
    "    final_residues = [res for res, _ in sorted_residue_distances[:num_residues]]\n",
    "\n",
    "    # Create a residue number to distance dictionary\n",
    "    pos_to_distance_dict = {}\n",
    "    for i, j in zip(final_residues, sorted_distances):\n",
    "        pos_to_distance_dict[i.id[1]] = j\n",
    "\n",
    "# Get the fasta file\n",
    "sequences = fasta_to_dict(fasta_filename)\n",
    "\n",
    "# Build the mapping dictionary\n",
    "mapping = build_mapping(sequences[query_id])\n",
    "\n",
    "# Get positions from the residues we identified from the protein structure\n",
    "query_positions = [res.id[1] for res in final_residues]\n",
    "\n",
    "# Get residues from other sequences in the alignment\n",
    "residues_from_alignment = get_corresponding_residues(sequences, mapping, query_positions)\n",
    "\n",
    "residue_data = []\n",
    "for res in final_residues:\n",
    "    res_data = {\n",
    "        'Chain': res.parent.id,\n",
    "        'Residue Name': aa_dict[res.resname],\n",
    "        'Residue Number': int(res.id[1]),\n",
    "        'Residue Distance': \"{:.1f}\".format(pos_to_distance_dict[res.id[1]])\n",
    "    }\n",
    "    for seq_id, res_list in residues_from_alignment.items():\n",
    "        pos = query_positions.index(res.id[1])  # index of the residue in the query_positions list\n",
    "        res_data[seq_id] = res_list[pos]  # This adds a column named seq_id with the corresponding residue as the value\n",
    "    residue_data.append(res_data)\n",
    "\n",
    "df = pd.DataFrame(residue_data)\n",
    "\n",
    "# Customized output:\n",
    "# Order: 'AncAT4', 'AncAT5', 'KAH6850668.1', 'KAH6641043.1', 'CazE',  'AncAT6',\tRYP17742.1\tTpAT\tKAF7554211.1\n",
    "df_cutomized = df[['Residue Number', 'Residue Distance', 'Anc451', 'Anc452', 'KAH6850668.1', 'KAH6641043.1', 'XP_001225293.1', 'Anc454', 'RYP17742.1', 'XP_046069174.1', 'KAF7554211.1']]\n",
    "\n",
    "# Initialize a list to hold the sum of differences, with a length that matches the number of DataFrame columns\n",
    "sum_of_differences = [None] * 11\n",
    "\n",
    "# Function to count differences between two values\n",
    "def count_differences(value1, value2):\n",
    "    return sum(a != b for a, b in zip(value1, value2))\n",
    "\n",
    "# Calculate the sum of differences and store them in the list\n",
    "for i in range(3, 11):  # Assuming you want to compare columns 4-11 with column 3\n",
    "    # Calculate the differences row-wise and sum them\n",
    "    sum_of_differences[i] = df_cutomized.apply(lambda row: count_differences(row.iloc[2], row.iloc[i]), axis=1).sum()\n",
    "\n",
    "# Append the sum of differences as a new row to the DataFrame\n",
    "df_cutomized = df_cutomized.append(pd.Series(sum_of_differences, index=df_cutomized.columns), ignore_index=True)\n",
    "\n",
    "df_cutomized.to_excel('../output/active_site_comparison.xlsx')\n",
    "df_cutomized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
